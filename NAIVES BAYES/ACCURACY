
import numpy as np


import random

def my_train_test_split(X, y, test_size=0.2, random_state=None):
    """Splits data into random train and test subsets.

    Args:
        X: The feature data (list or numpy array).
        y: The target labels (list or numpy array).
        test_size: Proportion of the dataset to include in the test split (default 0.2).
        random_state: Controls the shuffling applied to the data before applying the split (default None).

    Returns:
        X_train, X_test, y_train, y_test: The split data.
    """

    if random_state is not None:
        random.seed(random_state)  # Set random seed for reproducibility

    dataset = list(zip(X, y))  # Combine features and labels
    random.shuffle(dataset)  # Shuffle the dataset

    split_index = int(len(dataset) * (1 - test_size))  # Calculate split index
    X_train, y_train = zip(*dataset[:split_index])  # Extract training data
    X_test, y_test = zip(*dataset[split_index:])  # Extract testing data

    return list(X_train), list(X_test), list(y_train), list(y_test)  # Convert back to lists

class GaussianNaiveBayes:
    def fit(self, X, y):
        X = np.array(X)
        y = np.array(y)
        self.classes = np.unique(y)
        self.mean = {}
        self.var = {}
        self.priors = {}

        for cls in self.classes:
            X_c = X[y == cls]
            self.mean[cls] = np.mean(X_c, axis=0)
            self.var[cls] = np.var(X_c, axis=0) + 1e-9  # prevent division by zero
            self.priors[cls] = X_c.shape[0] / X.shape[0]

    def gaussian_prob(self, class_idx, x):
        mean = self.mean[class_idx]
        var = self.var[class_idx]
        numerator = np.exp(-((x - mean) ** 2) / (2 * var))
        denominator = np.sqrt(2 * np.pi * var)
        return numerator / denominator

    def predict(self, X):
        X = np.array(X)
        predictions = []
        for x in X:
            posteriors = []
            for cls in self.classes:
                prior = np.log(self.priors[cls])
                class_likelihood = np.sum(np.log(self.gaussian_prob(cls, x)))
                posterior = prior + class_likelihood
                posteriors.append(posterior)
            predictions.append(self.classes[np.argmax(posteriors)])
        return predictions

def evaluate_naive_bayes(X, y):
    X = normalize_features(X)  # use your min-max function
    X = np.array(X)
    y = np.array(y)

    # Split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Train & Predict
    model = GaussianNaiveBayes()
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Accuracy
    acc = np.mean(y_pred == y_test)
    print(f"Naive Bayes Accuracy: {acc * 100:.2f}%")
# X, y = prepare_dataset("/content/drive/MyDrive/genres_original")  #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<,
evaluate_naive_bayes(X, y)
